# Assign_ULLM

[Course webbook](https://cogsciprag.github.io/Understanding-LLMs-course/intro.html) and its [github source](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main?tab=readme-ov-file) <br>
[Slides access shortcut](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/lectures) <br>
[Tutorials shortcut](https://github.com/CogSciPrag/Understanding-LLMs-course/tree/main/understanding-llms/tutorials) <br>

#### Grades
6 CPs: 2/3 3-4 hw sets + 1/3 Final Exam at 23 Jul <br>

### Further Materials
Percy Liang's Lab [Standford CS324](https://stanford-cs324.github.io/winter2022/lectures/) <br>
Ryan Cotterell’s lab [S24](https://rycolab.io/classes/llm-s24/) <br>
Michael Franke [Pragmatic NLG with Neural Language Models](https://michael-franke.github.io/npNLG/000-intro.html) <br>
Andrej Karpathy [1hr Talk Intro to Large Language Models](https://www.youtube.com/watch?v=zjkBMFhNj_g) <br>
We also highly recommend Brian Christian’s book [The Alignment Problem](https://brianchristian.org/the-alignment-problem/) for a more general perspective on AI and computational CogSci.

## Introduction
1. A walk-through of NLP history
2. 

## Neural networks and small language models

## Transformers

## Large language models, prompting & fine-tuning

## Probing & attribution

## Evaluation

## Mechanistic interpretability

### Additional Notes
- Pytorch requries to specify which to track.
