{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Prompting & Generation with LMs (50 points)\n",
    "\n",
    "The second homework zooms in on the following skills: on gaining a deeper understanding of different state-of-the-art prompting techniques and training your critical conceptual thinking regarding research on LMs. \n",
    "\n",
    "### Logistics\n",
    "\n",
    "* submission deadline: June 2nd th 23:59 German time via Moodle\n",
    "  * please upload a **SINGLE .IPYNB FILE named Surname_FirstName_HW2.ipynb** containing your solutions of the homework.\n",
    "* please solve and submit the homework **individually**! \n",
    "* if you use Colab, to speed up the execution of the code on Colab, you can use the available GPU (if Colab resources allow). For that, before executing your code, navigate to Runtime > Change runtime type > GPU > Save.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Advanced prompting strategies (16 points)\n",
    "\n",
    "The lecture discussed various sophisticated ways of prompting language models for generating texts. Please answer the following questions about prompting techniques in context of different models, and write down your answers, briefly explaining them (max. 3 sentences). Feel free to actually implement some of the prompting strategies to play around with them and build your intuitions.\n",
    "\n",
    "> Consider the following language models: \n",
    "> * GPT-2, GPT-4, Vicuna (an instruction-tuned version of Llama) and Llama-2-7b-base.\n",
    ">  \n",
    "> Consider the following prompting / generation strategies: \n",
    "> * beam search, tree-of-thought reasoning, zero-shot CoT prompting, few-shot CoT prompting, few-shot prompting.\n",
    "> \n",
    "> For each model, which strategies do you think work well, and why? Do you think there are particular tasks or contexts, in which they work better, than in others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Answer\n",
    "1. GPT-2: Few-shot prompting works well for GPT-2 due to its lack of advanced reasoning capabilities, allowing it to mimic examples and patterns more effectively.\n",
    "2. GPT-4: Given the fact that GPT-4 is the a advanced, few-shot CoT prompting works well and is excellent for complex, multi-step tasks such as math problems or logical reasoning.\n",
    "3. Vicuna: Few-shot prompting and zero-shot CoT prompting are well-suited to Vicuna, which benefits from instruction-tuning to follow prompts effectively.\n",
    "4. Lama-2-7b-base: Few-shoting promoting is effective as the model is rather basic and needs guidance for examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Prompting for NLI & Multiple-choice QA (14 points)\n",
    "\n",
    "In this exercise, you can let your creativity flow -- your task is to come up with prompts for language models such that they achieve maximal accuracy on the following example tasks. Feel free to take inspiration from the in-class examples of the sentiment classification task. Also feel free to play around with the decoding scheme and see how it interacts with the different prompts.\n",
    "\n",
    "**TASK:**\n",
    "> Use the code that was introduced in the Intro to HF sheet to load the model and generate predictions from it with your sample prompts.\n",
    "> \n",
    "> * Please provide your code.\n",
    "> * Please report the best prompt that you found for each model and task (i.e., NLI and multiple choice QA), and the decoding scheme parameters that you used. \n",
    "> * Please write a brief summary of your explorations, stating what you tried, what worked (better), why you think that is.\n",
    "\n",
    "* Models: Pythia-410m, Pythia-1.4b\n",
    "* Tasks: please **test** the model on the following sentences and report the accuracy of the model with your best prompt and decoding configurations.\n",
    "  * Natural language inference: the task is to classify whether two sentences form a \"contradiction\" or an \"entailment\", or the relation is \"neutral\". The gold labels are provided for reference here, but obviously shouldn't be given to the model at test time.\n",
    "    * A person on a horse jumps over a broken down airplane. A person is training his horse for a competition. neutral\n",
    "    * A person on a horse jumps over a broken down airplane. A person is outdoors, on a horse. entailment\n",
    "    * Children smiling and waving at camera. There are children present. entailment\n",
    "    * A boy is jumping on skateboard in the middle of a red bridge. The boy skates down the sidewalk. contradiction\n",
    "    * An older man sits with his orange juice at a small table in a coffee shop while employees in bright colored shirts smile in the background. An older man drinks his juice as he waits for his daughter to get off work. neutral\n",
    "    * High fashion ladies wait outside a tram beside a crowd of people in the city. The women do not care what clothes they wear. contradiction\n",
    "  * Multiple choice QA: the task is to predict the correct answer option for the question, given the question and the options (like in the task of Ex. 3 of homework 1). The gold labels are provided for reference here, but obviously shouldn't be given to the model at test time.\n",
    "    * The only baggage the woman checked was a drawstring bag, where was she heading with it? [\"garbage can\", \"military\", \"jewelry store\", \"safe\", \"airport\"] -- airport\n",
    "    * To prevent any glare during the big football game he made sure to clean the dust of his what? [\"television\", \"attic\", \"corner\", \"they cannot clean corner and library during football match they cannot need that\", \"ground\"] -- television\n",
    "    * The president is the leader of what institution? [\"walmart\", \"white house\", \"country\", \"corporation\", \"government\"] -- country\n",
    "    * What kind of driving leads to accidents? [\"stressful\", \"dangerous\", \"fun\", \"illegal\", \"deadly\"] -- dangerous\n",
    "    * Can you name a good reason for attending school? [\"get smart\", \"boredom\", \"colds and flu\", \"taking tests\", \"spend time\"] -- \"get smart\"\n",
    "    * Stanley had a dream that was very vivid and scary. He had trouble telling it from what? [\"imagination\", \"reality\", \"dreamworker\", \"nightmare\", \"awake\"] -- reality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch \n",
    "import transformers \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# define computational device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Device: {device}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load pretrained tokeniser\n",
    "tokenizer_1_4b = AutoTokenizer.from_pretrained(\"EleutherAI/Pythia-1.4b\")\n",
    "\n",
    "# Load model\n",
    "model_1_4b = AutoModelForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/Pythia-1.4b\",\n",
    "    # trust_remote_code=True,\n",
    "    # torch_dtype=torch.float32, # always default\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load another model and pretrained tokeniser\n",
    "\n",
    "model_410m = AutoModelForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/Pythia-410m\",\n",
    "    # trust_remote_code=True,\n",
    "    # torch_dtype=torch.float32, # always default\n",
    ").to(device)\n",
    "\n",
    "tokenizer_410m = AutoTokenizer.from_pretrained(\"EleutherAI/Pythia-410m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 1. Natural Language Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sentences and labels\n",
    "statements = [\n",
    "    \"A person on a horse jumps over a broken down airplane. A person is training his horse for a competition.\",\n",
    "    \"A person on a horse jumps over a broken down airplane. A person is outdoors, on a horse.\",\n",
    "    \"Children smiling and waving at camera. There are children present.\",\n",
    "    \"A boy is jumping on skateboard in the middle of a red bridge. The boy skates down the sidewalk.\",\n",
    "    \"An older man sits with his orange juice at a small table in a coffee shop while employees in bright colored shirts smile in the background. An older man drinks his juice as he waits for his daughter to get off work.\",\n",
    "    \"High fashion ladies wait outside a tram beside a crowd of people in the city. The women do not care what clothes they wear.\",\n",
    "    ]\n",
    "\n",
    "labels = [\"neutral\", \"entailment\", \"entailment\", \"contradiction\", \"neutral\", \"contradiction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLI: zero-shot\n",
    "\n",
    "sentence_idx = 0\n",
    "zero_shot_full_prompt = \"Input: \" + statements[sentence_idx] + \" Output: \"\n",
    "# tokenize input\n",
    "input_ids_1_4b = tokenizer_1_4b(zero_shot_full_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "# generate predictions, model 1.4b\n",
    "zero_shot_prediction_1_4b = model_1_4b.generate(\n",
    "    input_ids_1_4b,\n",
    "    max_new_tokens=10,\n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    ")\n",
    "# model 410m\n",
    "input_ids_410m = tokenizer_410m(zero_shot_full_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "zero_shot_prediction_410m = model_410m.generate(\n",
    "    input_ids_410m,\n",
    "    max_new_tokens=10,\n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    ")\n",
    "\n",
    "# print\n",
    "print(tokenizer_410m.decode(zero_shot_prediction_410m[0], skip_special_tokens=False))\n",
    "print(\"Expected labels: \" + labels[sentence_idx])`\n",
    "print(tokenizer_1_4b.decode(zero_shot_prediction_1_4b[0], skip_special_tokens=False))\n",
    "print(\"Expected labels: \" + labels[sentence_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLI: few-shot\n",
    "few_shot_prompt = \"\"\"\n",
    "Input: A young girl practices ballet in a studio with mirrored walls. The girl is learning to dance. Output: entailment\n",
    "Input: A scientist conducts experiments in a laboratory filled with beakers and equipment. The scientist is studying chemical reactions. Output: entailment\n",
    "Input: A woman wearing a snorkel swims gracefully among colorful coral reefs. The woman is afraid of the ocean. Output: contradiction\n",
    "Input: A family enjoys a picnic in a sun-drenched meadow, surrounded by wildflowers. The family is indoors. Output: contradiction\n",
    "Input: A mother pushes her baby in a stroller through a bustling city street. The mother is caring for her child. Output: neutral\n",
    "Input: A student sits at a desk, surrounded by textbooks and notes, studying for an upcoming exam. The student is focused on academics. Output: neutral\n",
    "\"\"\"\n",
    "\n",
    "few_shot_full_prompt = few_shot_prompt + \"Input: \" + statements[sentence_idx] + \" Output: \"\n",
    "\n",
    "input_ids_1_4b = tokenizer_1_4b(few_shot_full_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "few_shot_prediction_1_4b = model_1_4b.generate(\n",
    "    input_ids_1_4b, \n",
    "    max_new_tokens=30, \n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    ")\n",
    "\n",
    "print(tokenizer_1_4b.decode(few_shot_prediction_1_4b[0], skip_special_tokens=False))\n",
    "print(\"Expected labels: \" + labels[sentence_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### DISCUSSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 2 MULTIPLE Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import additional libraries\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# set device again\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: The only baggage the woman checked was a drawstring bag, where was she heading with it? Options: garbage can, military, jewelry store, safe, airport Answer: airport\n",
      "Question: To prevent any glare during the big football game he made sure to clean the dust of his what? Options: television, attic, corner, they cannot clean corner and library during football match they cannot need that, ground Answer: television\n",
      "Question: The president is the leader of what institution? Options: walmart, white house, country, corporation, government Answer: country\n",
      "Question: What kind of driving leads to accidents? Options: stressful, dangerous, fun, illegal, deadly Answer: dangerous\n",
      "Question: Can you name a good reason for attending school? Options: get smart, boredom, colds and flu, taking tests, spend time Answer: get smart\n",
      "Question: Stanley had a dream that was very vivid and scary. He had trouble telling it from what? Options: imagination, reality, dreamworker, nightmare, awake Answer: reality\n"
     ]
    }
   ],
   "source": [
    "# define questions, options, and correct answers\n",
    "q_a_list = [\n",
    "    {\n",
    "        \"question\": \"The only baggage the woman checked was a drawstring bag, where was she heading with it?\",\n",
    "        \"options\": [\"garbage can\", \"military\", \"jewelry store\", \"safe\", \"airport\"],\n",
    "        \"correct answer\": \"airport\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"To prevent any glare during the big football game he made sure to clean the dust of his what?\",\n",
    "        \"options\": [\"television\", \"attic\", \"corner\", \"they cannot clean corner and library during football match they cannot need that\", \"ground\"],\n",
    "        \"correct answer\": \"television\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"The president is the leader of what institution?\",\n",
    "        \"options\": [\"walmart\", \"white house\", \"country\", \"corporation\", \"government\"],\n",
    "        \"correct answer\": \"country\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What kind of driving leads to accidents?\",\n",
    "        \"options\": [\"stressful\", \"dangerous\", \"fun\", \"illegal\", \"deadly\"],\n",
    "        \"correct answer\": \"dangerous\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can you name a good reason for attending school?\",\n",
    "        \"options\": [\"get smart\", \"boredom\", \"colds and flu\", \"taking tests\", \"spend time\"],\n",
    "        \"correct answer\": \"get smart\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Stanley had a dream that was very vivid and scary. He had trouble telling it from what?\",\n",
    "        \"options\": [\"imagination\", \"reality\", \"dreamworker\", \"nightmare\", \"awake\"],\n",
    "        \"correct answer\": \"reality\"\n",
    "    }]\n",
    "\n",
    "# design prompts\n",
    "prompts = []\n",
    "for qa in q_a_list:\n",
    "    question = qa[\"question\"]\n",
    "    options = qa[\"options\"]\n",
    "    correct_answer = qa[\"correct answer\"]\n",
    "    prompt = f\"Question: {question} Options: {', '.join(options)} Answer: {correct_answer}\"\n",
    "    prompts.append(prompt)\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 37, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mappend(predicted_answer)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[0;32m---> 15\u001b[0m model_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_1_4b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_1_4b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_predictions)\n",
      "Cell \u001b[0;32mIn[95], line 8\u001b[0m, in \u001b[0;36mget_predictions\u001b[0;34m(model, tokenizer, prompts)\u001b[0m\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, truncation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# use model to predict\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# decode predictions to text\u001b[39;00m\n\u001b[1;32m     10\u001b[0m predicted_answer \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/utils.py:1499\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1494\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `generation_config` defines a `cache_implementation` that is not compatible with this model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1495\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Make sure it has a `_setup_cache` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1496\u001b[0m             )\n\u001b[1;32m   1497\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_cache(cache_cls, max_batch_size\u001b[38;5;241m=\u001b[39mbatch_size, max_cache_len\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m-> 1499\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# 7. determine generation mode\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m generation_mode \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mget_generation_mode(assistant_model)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/utils.py:1149\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1148\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1153\u001b[0m     )\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1159\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 37, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "# define a function taking model and prompts as inputs and return predictions\n",
    "def get_predictions(model, tokenizer, prompts):\n",
    "    predictions = []\n",
    "    for prompt in prompts:\n",
    "        # use tokeniser to decode\n",
    "        inputs = tokenizer(prompt, return_tensors = \"pt\", max_length = 512, truncation = True)\n",
    "        # use model to predict\n",
    "        outputs = model.generate(**inputs)\n",
    "        # decode predictions to text\n",
    "        predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "        predictions.append(predicted_answer)\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "model_predictions = get_predictions(model_1_4b, tokenizer_1_4b, prompts)\n",
    "print(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "max_length = 128  # Adjust as needed\n",
    "dataset = QuestionAnswerDataset(questions, options, answers, tokenizer_1_4b, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 8  # Adjust as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AdamW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model_1_4b \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEleutherAI/Pythia-1.4b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define optimizer and loss function\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdamW\u001b[49m(model_1_4b\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m)\n\u001b[1;32m      6\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AdamW' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model_1_4b = AutoModelForCausalLM.from_pretrained(\"EleutherAI/Pythia-1.4b\").to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model_1_4b.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train model\n",
    "train_model(model_1_4b, train_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DISCUSSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: First neural LM (20 points)\n",
    "\n",
    "Next to reading and understanding package documentations, a key skill for NLP researchers and practitioners is reading and critically assessing NLP literature. The density, but also the style of NLP literature has undergone a significant shift in the recent years with increasing acceleration of progress. Your task in this exercise is to read a paper about one of the first successful neural langauge models, understand its key architectural components and compare how these key components have evolved in modern systems that were discussed in the lecture. \n",
    "\n",
    "> Specifically, please read this paper and answer the following questions: [Bengio et al. (2003)](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    ">\n",
    "> * How were words / tokens represented? What is the difference / similarity to modern LLMs?\n",
    "> * How was the context represented? What is the difference / similarity to modern LLMs?\n",
    "> * What is the curse of dimensionality? Give a concrete example in the context of language modeling.\n",
    "> * Which training data was used? What is the difference / similarity to modern LLMs?\n",
    "> * Which components of the Bengio et al. (2003) model (if any) can be found in modern LMs?\n",
    "> \n",
    "> * Please formulate one question about the paper (not the same as the questions above) and post it to the dedicated **Forum** space, and **answer 1 other question** about the paper.\n",
    "\n",
    "Furthermore, your task is to carefully dissect the paper by Bengio et al. (2003) and analyse its structure and style in comparison to another more recent paper:  [Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)\n",
    "\n",
    "**TASK:**\n",
    "\n",
    "> For each section of the Bengio et al. (2003) paper, what are key differences between the way it is written, the included contents, to the BERT paper (Devlin et al., 2019)? What are key similarities? Write max. 2 sentences per section.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
