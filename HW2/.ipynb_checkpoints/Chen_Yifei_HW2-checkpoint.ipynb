{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Prompting & Generation with LMs (50 points)\n",
    "\n",
    "The second homework zooms in on the following skills: on gaining a deeper understanding of different state-of-the-art prompting techniques and training your critical conceptual thinking regarding research on LMs. \n",
    "\n",
    "### Logistics\n",
    "\n",
    "* submission deadline: June 2nd th 23:59 German time via Moodle\n",
    "  * please upload a **SINGLE .IPYNB FILE named Surname_FirstName_HW2.ipynb** containing your solutions of the homework.\n",
    "* please solve and submit the homework **individually**! \n",
    "* if you use Colab, to speed up the execution of the code on Colab, you can use the available GPU (if Colab resources allow). For that, before executing your code, navigate to Runtime > Change runtime type > GPU > Save.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Advanced prompting strategies (16 points)\n",
    "\n",
    "The lecture discussed various sophisticated ways of prompting language models for generating texts. Please answer the following questions about prompting techniques in context of different models, and write down your answers, briefly explaining them (max. 3 sentences). Feel free to actually implement some of the prompting strategies to play around with them and build your intuitions.\n",
    "\n",
    "> Consider the following language models: \n",
    "> * GPT-2, GPT-4, Vicuna (an instruction-tuned version of Llama) and Llama-2-7b-base.\n",
    ">  \n",
    "> Consider the following prompting / generation strategies: \n",
    "> * beam search, tree-of-thought reasoning, zero-shot CoT prompting, few-shot CoT prompting, few-shot prompting.\n",
    "> \n",
    "> For each model, which strategies do you think work well, and why? Do you think there are particular tasks or contexts, in which they work better, than in others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Answer\n",
    "1. GPT-2: Few-shot prompting works well for GPT-2 due to its lack of advanced reasoning capabilities, allowing it to mimic examples and patterns more effectively.\n",
    "2. GPT-4: Given the fact that GPT-4 is the a advanced, few-shot CoT prompting works well and is excellent for complex, multi-step tasks such as math problems or logical reasoning.\n",
    "3. Vicuna: Few-shot prompting and zero-shot CoT prompting are well-suited to Vicuna, which benefits from instruction-tuning to follow prompts effectively.\n",
    "4. Lama-2-7b-base: Few-shoting promoting is effective as the model is rather basic and needs guidance for examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Prompting for NLI & Multiple-choice QA (14 points)\n",
    "\n",
    "In this exercise, you can let your creativity flow -- your task is to come up with prompts for language models such that they achieve maximal accuracy on the following example tasks. Feel free to take inspiration from the in-class examples of the sentiment classification task. Also feel free to play around with the decoding scheme and see how it interacts with the different prompts.\n",
    "\n",
    "**TASK:**\n",
    "> Use the code that was introduced in the Intro to HF sheet to load the model and generate predictions from it with your sample prompts.\n",
    "> \n",
    "> * Please provide your code.\n",
    "> * Please report the best prompt that you found for each model and task (i.e., NLI and multiple choice QA), and the decoding scheme parameters that you used. \n",
    "> * Please write a brief summary of your explorations, stating what you tried, what worked (better), why you think that is.\n",
    "\n",
    "* Models: Pythia-410m, Pythia-1.4b\n",
    "* Tasks: please **test** the model on the following sentences and report the accuracy of the model with your best prompt and decoding configurations.\n",
    "  * Natural language inference: the task is to classify whether two sentences form a \"contradiction\" or an \"entailment\", or the relation is \"neutral\". The gold labels are provided for reference here, but obviously shouldn't be given to the model at test time.\n",
    "    * A person on a horse jumps over a broken down airplane. A person is training his horse for a competition. neutral\n",
    "    * A person on a horse jumps over a broken down airplane. A person is outdoors, on a horse. entailment\n",
    "    * Children smiling and waving at camera. There are children present. entailment\n",
    "    * A boy is jumping on skateboard in the middle of a red bridge. The boy skates down the sidewalk. contradiction\n",
    "    * An older man sits with his orange juice at a small table in a coffee shop while employees in bright colored shirts smile in the background. An older man drinks his juice as he waits for his daughter to get off work. neutral\n",
    "    * High fashion ladies wait outside a tram beside a crowd of people in the city. The women do not care what clothes they wear. contradiction\n",
    "  * Multiple choice QA: the task is to predict the correct answer option for the question, given the question and the options (like in the task of Ex. 3 of homework 1). The gold labels are provided for reference here, but obviously shouldn't be given to the model at test time.\n",
    "    * The only baggage the woman checked was a drawstring bag, where was she heading with it? [\"garbage can\", \"military\", \"jewelry store\", \"safe\", \"airport\"] -- airport\n",
    "    * To prevent any glare during the big football game he made sure to clean the dust of his what? [\"television\", \"attic\", \"corner\", \"they cannot clean corner and library during football match they cannot need that\", \"ground\"] -- television\n",
    "    * The president is the leader of what institution? [\"walmart\", \"white house\", \"country\", \"corporation\", \"government\"] -- country\n",
    "    * What kind of driving leads to accidents? [\"stressful\", \"dangerous\", \"fun\", \"illegal\", \"deadly\"] -- dangerous\n",
    "    * Can you name a good reason for attending school? [\"get smart\", \"boredom\", \"colds and flu\", \"taking tests\", \"spend time\"] -- \"get smart\"\n",
    "    * Stanley had a dream that was very vivid and scary. He had trouble telling it from what? [\"imagination\", \"reality\", \"dreamworker\", \"nightmare\", \"awake\"] -- reality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-410m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-1.4b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Pythia-410m: 0.33\n",
      "Accuracy of Pythia-1.4b: 0.33\n"
     ]
    }
   ],
   "source": [
    "# TASK 1. Natural Language Inference\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load the models and tokenizer\n",
    "model_name_410m = \"EleutherAI/pythia-410m\"\n",
    "model_name_1_4b = \"EleutherAI/pythia-1.4b\"\n",
    "\n",
    "tokenizer_410m = AutoTokenizer.from_pretrained(model_name_410m)\n",
    "model_410m = AutoModelForSequenceClassification.from_pretrained(model_name_410m, num_labels=3)\n",
    "\n",
    "tokenizer_1_4b = AutoTokenizer.from_pretrained(model_name_1_4b)\n",
    "model_1_4b = AutoModelForSequenceClassification.from_pretrained(model_name_1_4b, num_labels=3)\n",
    "\n",
    "# Set the padding token\n",
    "if tokenizer_410m.pad_token is None:\n",
    "    tokenizer_410m.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model_410m.resize_token_embeddings(len(tokenizer_410m))\n",
    "\n",
    "if tokenizer_1_4b.pad_token is None:\n",
    "    tokenizer_1_4b.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model_1_4b.resize_token_embeddings(len(tokenizer_1_4b))\n",
    "\n",
    "# Define the test sentences and labels\n",
    "sentences = [\n",
    "    (\"A person on a horse jumps over a broken down airplane.\", \"A person is training his horse for a competition.\", \"neutral\"),\n",
    "    (\"A person on a horse jumps over a broken down airplane.\", \"A person is outdoors, on a horse.\", \"entailment\"),\n",
    "    (\"Children smiling and waving at camera.\", \"There are children present.\", \"entailment\"),\n",
    "    (\"A boy is jumping on skateboard in the middle of a red bridge.\", \"The boy skates down the sidewalk.\", \"contradiction\"),\n",
    "    (\"An older man sits with his orange juice at a small table in a coffee shop while employees in bright colored shirts smile in the background.\", \"An older man drinks his juice as he waits for his daughter to get off work.\", \"neutral\"),\n",
    "    (\"High fashion ladies wait outside a tram beside a crowd of people in the city.\", \"The women do not care what clothes they wear.\", \"contradiction\")\n",
    "]\n",
    "\n",
    "# Define a function to classify sentence pairs\n",
    "def classify_nli(model, tokenizer, premise, hypothesis, max_length=128):\n",
    "    inputs = tokenizer(premise, hypothesis, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    return [\"entailment\", \"neutral\", \"contradiction\"][predicted_class_id]\n",
    "\n",
    "# Test the models and compute accuracy\n",
    "def evaluate_model(model, tokenizer):\n",
    "    correct = 0\n",
    "    for premise, hypothesis, gold_label in sentences:\n",
    "        prediction = classify_nli(model, tokenizer, premise, hypothesis)\n",
    "        if prediction == gold_label:\n",
    "            correct += 1\n",
    "    return correct / len(sentences)\n",
    "\n",
    "# Evaluate both models\n",
    "accuracy_410m = evaluate_model(model_410m, tokenizer_410m)\n",
    "accuracy_1_4b = evaluate_model(model_1_4b, tokenizer_1_4b)\n",
    "\n",
    "print(f\"Accuracy of Pythia-410m: {accuracy_410m:.2f}\")\n",
    "print(f\"Accuracy of Pythia-1.4b: {accuracy_1_4b:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2\n",
    "\n",
    "# Define the test questions and answer options\n",
    "questions = [\n",
    "    (\"The only baggage the woman checked was a drawstring bag, where was she heading with it?\", [\"garbage can\", \"military\", \"jewelry store\", \"safe\", \"airport\"], \"airport\"),\n",
    "    (\"To prevent any glare during the big football game he made sure to clean the dust of his what?\", [\"television\", \"attic\", \"corner\", \"they cannot clean corner and library during football match they cannot need that\", \"ground\"], \"television\"),\n",
    "    (\"The president is the leader of what institution?\", [\"walmart\", \"white house\", \"country\", \"corporation\", \"government\"], \"country\"),\n",
    "    (\"What kind of driving leads to accidents?\", [\"stressful\", \"dangerous\", \"fun\", \"illegal\", \"deadly\"], \"dangerous\"),\n",
    "    (\"Can you name a good reason for attending school?\", [\"get smart\", \"boredom\", \"colds and flu\", \"taking tests\", \"spend time\"], \"get smart\"),\n",
    "    (\"Stanley had a dream that was very vivid and scary. He had trouble telling it from what?\", [\"imagination\", \"reality\", \"dreamworker\", \"nightmare\", \"awake\"], \"reality\")\n",
    "]\n",
    "\n",
    "# Define a function to generate the prompt and predict the answer\n",
    "def predict_answer(model, tokenizer, question, options):\n",
    "    prompt = f\"Question: {question}\\nOptions:\\n\"\n",
    "    for i, option in enumerate(options):\n",
    "        prompt += f\"{i + 1}. {option}\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(**inputs, max_length=len(inputs['input_ids'][0]) + 1, num_return_sequences=1)\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = output_text.split(\"Answer:\")[1].strip().split()[0]\n",
    "    \n",
    "    if answer.isdigit() and int(answer) <= len(options):\n",
    "        return options[int(answer) - 1]\n",
    "    return None\n",
    "\n",
    "# Test the models and compute accuracy\n",
    "def evaluate_model(model, tokenizer):\n",
    "    correct = 0\n",
    "    for question, options, gold_label in questions:\n",
    "        prediction = predict_answer(model, tokenizer, question, options)\n",
    "        if prediction == gold_label:\n",
    "            correct += 1\n",
    "    return correct / len(questions)\n",
    "\n",
    "# Evaluate both models\n",
    "accuracy_410m = evaluate_model(model_410m, tokenizer_410m)\n",
    "accuracy_1_4b = evaluate_model(model_1_4b, tokenizer_1_4b)\n",
    "\n",
    "print(f\"Accuracy of Pythia-410m: {accuracy_410m:.2f}\")\n",
    "print(f\"Accuracy of Pythia-1.4b: {accuracy_1_4b:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: First neural LM (20 points)\n",
    "\n",
    "Next to reading and understanding package documentations, a key skill for NLP researchers and practitioners is reading and critically assessing NLP literature. The density, but also the style of NLP literature has undergone a significant shift in the recent years with increasing acceleration of progress. Your task in this exercise is to read a paper about one of the first successful neural langauge models, understand its key architectural components and compare how these key components have evolved in modern systems that were discussed in the lecture. \n",
    "\n",
    "> Specifically, please read this paper and answer the following questions: [Bengio et al. (2003)](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    ">\n",
    "> * How were words / tokens represented? What is the difference / similarity to modern LLMs?\n",
    "> * How was the context represented? What is the difference / similarity to modern LLMs?\n",
    "> * What is the curse of dimensionality? Give a concrete example in the context of language modeling.\n",
    "> * Which training data was used? What is the difference / similarity to modern LLMs?\n",
    "> * Which components of the Bengio et al. (2003) model (if any) can be found in modern LMs?\n",
    "> \n",
    "> * Please formulate one question about the paper (not the same as the questions above) and post it to the dedicated **Forum** space, and **answer 1 other question** about the paper.\n",
    "\n",
    "Furthermore, your task is to carefully dissect the paper by Bengio et al. (2003) and analyse its structure and style in comparison to another more recent paper:  [Devlin et al. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)\n",
    "\n",
    "**TASK:**\n",
    "\n",
    "> For each section of the Bengio et al. (2003) paper, what are key differences between the way it is written, the included contents, to the BERT paper (Devlin et al., 2019)? What are key similarities? Write max. 2 sentences per section.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
