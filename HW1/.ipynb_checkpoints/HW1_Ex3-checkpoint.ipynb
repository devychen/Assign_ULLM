{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task overview <br>\n",
    "**Goal**: Practice fine-tuning a pre-trained LM (GPT2-small) on the particular task of commonsense question answering (QA) <br>\n",
    "**Dataset**: [CommonsenseQA](https://huggingface.co/datasets/tau/commonsense_qa) <br>\n",
    "**Description** <br>\n",
    "- Evaluate the performance of the model on testset over training set. <br>\n",
    "- Monitor: \n",
    "    - Whether the modelâ€™s performance is improving; \n",
    "    - Compare the performance of the base pretrained GPT-2 and the fine-tuned model \n",
    "- Steps:\n",
    "    1. Data preparation. Simiar to [sheet 1.1](https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/01-introduction.html#main-training-data-processing-steps) and [sheet 2.3](https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/02c-MLP-pytorch.html#preparing-the-training-data)\n",
    "    2. Load the pretrained GPT-2 model\n",
    "    3. Set up training pipiline. Steps similar to [sheet 2.5](https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/02e-intro-to-hf.html)\n",
    "    4. Run the training while tracking the losses\n",
    "    5. Save plot of losses for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.30.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# pkg preparation\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "!pip3 install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Data Preparation\n",
    "1. Acquiring data <br>\n",
    "2. Minimally exploring dataset <br>\n",
    "3. Cleaning / wrangling data (combines step 4 from sheet 1.1 and step 1.1 above) <br>\n",
    "4. Splitting data into training and test set (we will not do any hyperparam tuning) (we don't need further training set wrangling) <br>\n",
    "5. Tokenizing data and making sure it can be batched (i.e., conversted into 2d tensors), this will also happen in our custom Dataset class (common practice when working with text data) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keys are:\n",
      " dict_keys(['train', 'validation', 'test'])\n",
      "A sample:\n",
      " {'id': '075e483d21c29a511267ef62bedc0461', 'question': 'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?', 'question_concept': 'punishing', 'choices': {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']}, 'answerKey': 'A'}\n"
     ]
    }
   ],
   "source": [
    "# downaload dataset from HF\n",
    "dataset = load_dataset(\"tau/commonsense_qa\")\n",
    "# inspect dataset, print all keys\n",
    "print(\"The keys are:\\n\", dataset.keys())\n",
    "# print a sample from the dataset\n",
    "print(\"A sample:\\n\", dataset[\"train\"][0]) # CODE\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# set padding side to be left because we are doing causal LM\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694267408e734c6187c23318c862905e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9741 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5c84e9022940dd886d954eaf95a0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480c6031e05f463c91cae119f27783b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def massage_input_text(example):\n",
    "    \"\"\"\n",
    "    Helper for converting input examples which have \n",
    "    a separate qquestion, labels, answer options\n",
    "    into a single string.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    example: dict\n",
    "        Sample input from the dataset which contains the \n",
    "        question, answer labels (e.g. A, B, C, D),\n",
    "        the answer options for the question, and which \n",
    "        of the answers is correct.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    input_text: str\n",
    "        Formatted training text which contains the question,\n",
    "        the forwatted answer options (e.g., 'A. <option 1> B. <option 2>' etc)\n",
    "        and the ground truth answer.\n",
    "    \"\"\"\n",
    "    # combine each label with its corresponding text\n",
    "    answer_options_list = list(zip(\n",
    "        example[\"choices\"][\"label\"],\n",
    "        example[\"choices\"][\"text\"]\n",
    "    ))\n",
    "    # join each label and text with . and space\n",
    "    answer_options = \" \".join([f\"{label}. {text}\" for label, text in answer_options_list]) # CODE\n",
    "    # join the list of options with spaces into single string\n",
    "    answer_options_string = \" \".join(answer_options.split()) # CODE\n",
    "    # combine question and answer options\n",
    "    input_text = example[\"question\"] + \" \" + answer_options_string\n",
    "    # append the true answer with a new line, \"Answer: \" and the label\n",
    "    input_text += \"\\nAnswer: \" + example[\"answerKey\"]\n",
    "\n",
    "    return input_text\n",
    "\n",
    "# process input texts of train and test sets\n",
    "massaged_datasets = dataset.map(\n",
    "    lambda example: {\n",
    "        \"text\": massage_input_text(example)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample from the preprocessed data:\n",
      " {'id': '075e483d21c29a511267ef62bedc0461', 'question': 'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?', 'question_concept': 'punishing', 'choices': {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']}, 'answerKey': 'A', 'text': 'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? A. ignore B. enforce C. authoritarian D. yell at E. avoid\\nAnswer: A'}\n"
     ]
    }
   ],
   "source": [
    "# inspect a sample from our preprocessed data\n",
    "prep_data_sample = massaged_datasets[\"train\"][0] # CODE, modified\n",
    "print(\"A sample from the preprocessed data:\\n\", prep_data_sample) # CODE, modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonsenseQADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for CommonsenseQA dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            train_split, \n",
    "            test_split,\n",
    "            tokenizer,\n",
    "            max_length=64,\n",
    "            dataset_split=\"train\",\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the dataset object.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        train_split: dict\n",
    "            Training data dictionary with different columns.\n",
    "        test_split: dict\n",
    "            Test data dictionary with different columns.\n",
    "        tokenizer: Tokenizer\n",
    "            Initialized tokenizer for processing samples.\n",
    "        max_length: int\n",
    "            Maximal length of inputs. All inputs will be \n",
    "            truncated or padded to this length.\n",
    "        dataset_split: str\n",
    "            Specifies which split of the dataset to use. \n",
    "            Default is \"train\".\n",
    "        \"\"\"\n",
    "        self.train_split = train_split['text']\n",
    "        self.test_split = test_split['text']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.dataset_split = dataset_split\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Method returning the length of the training dataset.\n",
    "        \"\"\"\n",
    "        return len(self.train_split) if self.dataset_split == \"train\" else len(self.test_split) # CODE\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Method returning a single training example.\n",
    "        Note that it also tokenizes, truncates or pads the input text.\n",
    "        Further, it creates a mask tensor for the input text which \n",
    "        is used for causal masking in the transformer model.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        idx: int\n",
    "            Index of training sample to be retrieved from the data.\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        tokenized_input: dict\n",
    "            Dictionary with input_ids (torch.Tensor) and an attention_mask\n",
    "            (torch.Tensor).\n",
    "        \"\"\"\n",
    "        # retrieve a training sample at the specified index idx\n",
    "        # HINT: note that this might depend on self.dataset_split\n",
    "        input_text = self.train_split[idx] if self.dataset_split == \"train\" else self.test_split[idx] # CODE\n",
    "        tokenized_input = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length = self.max_length, # CODE\n",
    "            padding = \"max_length\",\n",
    "            truncation = True,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "        tokenized_input[\"attention_mask\"] = (tokenized_input[\"input_ids\"] != tokenizer.pad_token_id).long()\n",
    "        return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# move to accelerated device \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Device: {device}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Initialise Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c29c80a8bef4972970790fe31382422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load pretrained gpt2 for HF\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\") # CODE\n",
    "# print num of trainable parameters\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f} M parameters\")\n",
    "\n",
    "# Hint: If you run out of memory while trying to run the training, try decreasing the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Set up configurations required for the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# instantiate dataset with the downloaded commonsense_qa data \u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m CommonsenseQADataset(\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain_split\u001b[49m, test_split, tokenizer, \n\u001b[1;32m      4\u001b[0m     max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, \n\u001b[1;32m      5\u001b[0m     dataset_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# CODE\u001b[39;00m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# instantiate test dataset with the downloaded commonsense_qa data\u001b[39;00m\n\u001b[1;32m      8\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CommonsenseQADataset(\n\u001b[1;32m      9\u001b[0m     train_split, test_split, tokenizer, \n\u001b[1;32m     10\u001b[0m     max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, \n\u001b[1;32m     11\u001b[0m     dataset_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# CODE\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_split' is not defined"
     ]
    }
   ],
   "source": [
    "# instantiate dataset with the downloaded commonsense_qa data \n",
    "train_dataset = CommonsenseQADataset(\n",
    "    train_split, test_split, tokenizer, \n",
    "    max_length = 64, \n",
    "    dataset_split = \"train\" # CODE\n",
    ")\n",
    "# instantiate test dataset with the downloaded commonsense_qa data\n",
    "test_dataset = CommonsenseQADataset(\n",
    "    train_split, test_split, tokenizer, \n",
    "    max_length = 64, \n",
    "    dataset_split = \"test\" # CODE\n",
    ")\n",
    "# create a DataLoader for the dataset\n",
    "# the data loader will automatically batch the data\n",
    "# and iteratively return training examples (question answer pairs) in batches\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True\n",
    ")\n",
    "# create a DataLoader for the test dataset\n",
    "# reason for separate data loader is that we want to\n",
    "# be able to use a different index for retreiving the test batches\n",
    "# we might also want to use a different batch size etc.\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Run the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: for implementing the forward pass and loss computation, carefully look at the exercise sheets \n",
    "# and the links to examples in HF tutorials.\n",
    "\n",
    "# put the model in training mode\n",
    "model.train()\n",
    "# move the model to the device (e.g. GPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# trianing configutations \n",
    "# feel free to play around with these\n",
    "epochs  = 1\n",
    "train_steps =  len(train_dataset) // 32\n",
    "print(\"Number of training steps: \", train_steps)\n",
    "# number of test steps to perform every 10 training steps\n",
    "# (smaller that the entore test split for reasons of comp. time)\n",
    "num_test_steps = 5\n",
    "\n",
    "# define optimizer and learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4) \n",
    "# define some variables to accumulate the losses\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "# iterate over epochs\n",
    "for e in range(epochs):\n",
    "    # iterate over training steps\n",
    "    for i in tqdm(range(train_steps)):\n",
    "        # get a batch of data\n",
    "        x = next(iter(dataloader))\n",
    "        # move the data to the device (GPU)\n",
    "        x = {key: val.to(device) for key, val in x.items()}  # Move batch to device # CODE\n",
    "\n",
    "        # forward pass through the model\n",
    "        ### YOUR CODE HERE ###\n",
    "        outputs = model(\n",
    "            ### YOUR CODE HERE ####\n",
    "        )\n",
    "        # get the loss\n",
    "        loss = outputs.loss # CODE\n",
    "        # backward pass\n",
    "        loss.backward() # CODE\n",
    "        losses.append(loss.item())\n",
    "        # update the parameters of the model\n",
    "        optimizer.step() # CODE\n",
    "\n",
    "        # zero out gradient for next step\n",
    "        optimizer.zero_grad() # CODE\n",
    "\n",
    "        # evaluate on test set every 10 steps\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {e}, step {i}, loss {loss.item()}\")\n",
    "            # track test loss for the evaluation iteration\n",
    "            test_loss = 0\n",
    "            for j in range(num_test_steps):\n",
    "                # get test batch\n",
    "                x_test = next(iter(test_dataloader))\n",
    "                x_test = x_test.to(device)\n",
    "                with torch.no_grad():\n",
    "                    test_outputs = model(\n",
    "                        **x_test # CODE\n",
    "                    )\n",
    "                test_loss += test_outputs.loss # CODE\n",
    "                \n",
    "            test_losses.append(test_loss / num_test_steps)\n",
    "            print(\"Test loss: \", test_loss/num_test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Plot the fine-tuning loss and MAKE SURE TO SAVE IT AND SUBMIT IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training losses on x axis\n",
    "plt.plot(loss) # CODE\n",
    "plt.xlabel(\"Training steps\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a few predictions on the eval dataset to see what the model predicts\n",
    "\n",
    "# construct a list of questions without the ground truth label\n",
    "# and compare prediction of the model with the ground truth\n",
    "\n",
    "def construct_test_samples(example):\n",
    "    \"\"\"\n",
    "    Helper for converting input examples which have \n",
    "    a separate qquestion, labels, answer options\n",
    "    into a single string for testing the model.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    example: dict\n",
    "        Sample input from the dataset which contains the \n",
    "        question, answer labels (e.g. A, B, C, D),\n",
    "        the answer options for the question, and which \n",
    "        of the answers is correct.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    input_text: str, str\n",
    "        Tuple: Formatted test text which contains the question,\n",
    "        the forwatted answer options (e.g., 'A. <option 1> B. <option 2>' etc); \n",
    "        the ground truth answer label only.\n",
    "    \"\"\"\n",
    "\n",
    "    answer_options_list = list(zip(\n",
    "        example[\"choices\"][\"label\"],\n",
    "        example[\"choices\"][\"text\"]\n",
    "    ))\n",
    "    # join each label and text with . and space\n",
    "    answer_options = \" \".join([f\"{label}.{text}\" for label, text in answer_options_list]) # CODE\n",
    "    # join the list of options with spaces into single string\n",
    "    answer_options_string = \" \".join(answer_options.split()) # CODE\n",
    "    # combine question and answer options\n",
    "    input_text = example[\"question\"] + \" \" + answer_options_string\n",
    "    # create the test input text which should be:\n",
    "    # the input text, followed by the string \"Answer: \"\n",
    "    # we don't need to append the ground truth answer since we are creating test inputs\n",
    "    # and the answer should be predicted.\n",
    "    input_text += \"\\nAnswer: \" # CODE\n",
    "\n",
    "    return input_text, example[\"answerKey\"]\n",
    "\n",
    "test_samples = [construct_test_samples(dataset[\"validation\"][i]) for i in range(10)]\n",
    "test_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One More Step: Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set it to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "for sample in test_samples:\n",
    "    input_text = sample[0]\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(\n",
    "        input_ids.input_ids,\n",
    "        attention_mask = input_ids.attention_mask,\n",
    "        max_new_tokens=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.4,\n",
    "    )\n",
    "    prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    predictions.append((input_text, prediction, sample[1]))\n",
    "\n",
    "print(\"Predictions of trained model \", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
